{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "from itertools import islice\n",
    "import concurrent.futures\n",
    "import yaml\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import pyperclip\n",
    "\n",
    "# OpenAI Libraries\n",
    "from openai import OpenAI\n",
    "\n",
    "# Google Cloud Identity and Credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import functions_v1\n",
    "from google.api_core.exceptions import Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this as a variable to reference in function app in later step\n",
    "openai_api_key = json.load(\n",
    "    open(\"openai.json\")\n",
    ")[\"key\"]\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "embeddings_model = \"text-embedding-3-small\"  # We'll use this by default, but you can change to your text-embedding-3-large if desired\n",
    "\n",
    "# Use default credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    # os.path.join(os.path.dirname(os.path.abspath(__file__)), \"google.json\")\n",
    "    \"google.json\"\n",
    ")\n",
    "project_id = \"cse-144-project\"\n",
    "region = \"us-central1\"  # e.g: \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n",
    "    # batched('ABCDEFG', 3) --> ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be at least one\")\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def chunked_tokens(text, chunk_length, encoding_name=\"cl100k_base\"):\n",
    "    # Get the encoding object for the specified encoding name. OpenAI's tiktoken library, which is used in this notebook, currently supports two encodings: 'bpe' and 'cl100k_base'. The 'bpe' encoding is used for GPT-3 and earlier models, while 'cl100k_base' is used for newer models like GPT-4.\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # Encode the input text into tokens\n",
    "    tokens = encoding.encode(text)\n",
    "    # Create an iterator that yields chunks of tokens of the specified length\n",
    "    chunks_iterator = batched(tokens, chunk_length)\n",
    "    # Yield each chunk from the iterator\n",
    "    yield from chunks_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING = \"cl100k_base\"\n",
    "\n",
    "\n",
    "def generate_embeddings(text, model):\n",
    "    # Generate embeddings for the provided text using the specified model\n",
    "    embeddings_response = openai_client.embeddings.create(model=model, input=text)\n",
    "    # Extract the embedding data from the response\n",
    "    embedding = embeddings_response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def len_safe_get_embedding(\n",
    "    text,\n",
    "    model=embeddings_model,\n",
    "    max_tokens=EMBEDDING_CTX_LENGTH,\n",
    "    encoding_name=EMBEDDING_ENCODING,\n",
    "):\n",
    "    # Initialize lists to store embeddings and corresponding text chunks\n",
    "    chunk_embeddings = []\n",
    "    chunk_texts = []\n",
    "    # Iterate over chunks of tokens from the input text\n",
    "    for chunk in chunked_tokens(\n",
    "        text, chunk_length=max_tokens, encoding_name=encoding_name\n",
    "    ):\n",
    "        # Generate embeddings for each chunk and append to the list\n",
    "        chunk_embeddings.append(generate_embeddings(chunk, model=model))\n",
    "        # Decode the chunk back to text and append to the list\n",
    "        chunk_texts.append(tiktoken.get_encoding(encoding_name).decode(chunk))\n",
    "    # Return the list of chunk embeddings and the corresponding text chunks\n",
    "    return chunk_embeddings, chunk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"authentication\",\n",
    "    \"models\",\n",
    "    \"techniques\",\n",
    "    \"tools\",\n",
    "    \"setup\",\n",
    "    \"billing_limits\",\n",
    "    \"other\",\n",
    "]\n",
    "\n",
    "\n",
    "def categorize_text(text, categories):\n",
    "\n",
    "    # Create a prompt for categorization\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"You are an expert in LLMs, and you will be given text that corresponds to an article in OpenAI's documentation.\n",
    "         Categorize the document into one of these categories: {', '.join(categories)}. Only respond with the category name and nothing else.\"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    try:\n",
    "        # Call the OpenAI API to categorize the text\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\", messages=messages\n",
    "        )\n",
    "\n",
    "        # Extract the category from the response\n",
    "        category = response.choices[0].message.content\n",
    "        return category\n",
    "    except Exception as e:\n",
    "        print(f\"Error categorizing text: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def process_file(file_path, idx, categories, embeddings_model):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Processing file {idx + 1}: {file_name}\")\n",
    "\n",
    "    # Read text content from .txt files\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "    # Extract text content from .pdf files\n",
    "    # elif file_name.endswith(\".pdf\"):\n",
    "    #     text = extract_text_from_pdf(file_path)\n",
    "\n",
    "    title = file_name\n",
    "    # Generate embeddings for the title\n",
    "    title_vectors, title_text = len_safe_get_embedding(title, embeddings_model)\n",
    "    print(f\"Generated title embeddings for {file_name}\")\n",
    "\n",
    "    # Generate embeddings for the content\n",
    "    content_vectors, content_text = len_safe_get_embedding(text, embeddings_model)\n",
    "    print(f\"Generated content embeddings for {file_name}\")\n",
    "\n",
    "    category = categorize_text(\" \".join(content_text), categories)\n",
    "    print(f\"Categorized {file_name} as {category}\")\n",
    "\n",
    "    # Prepare the data to be appended\n",
    "    data = []\n",
    "    for i, content_vector in enumerate(content_vectors):\n",
    "        data.append(\n",
    "            {\n",
    "                \"id\": f\"{idx}_{i}\",\n",
    "                \"vector_id\": f\"{idx}_{i}\",\n",
    "                \"title\": title_text[0],\n",
    "                \"text\": content_text[i],\n",
    "                \"title_vector\": json.dumps(\n",
    "                    title_vectors[0]\n",
    "                ),  # Assuming title is short and has only one chunk\n",
    "                \"content_vector\": json.dumps(content_vector),\n",
    "                \"category\": category,\n",
    "            }\n",
    "        )\n",
    "        print(f\"Appended data for chunk {i + 1}/{len(content_vectors)} of {file_name}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1: latency-optimization.txtProcessing file 2: function-calling-run-example--polling.txt\n",
      "\n",
      "Processing file 3: fine-tuning.txt\n",
      "Processing file 4: hackathons.txt\n",
      "Processing file 5: tier-two.txt\n",
      "Processing file 6: images-node-tips.txt\n",
      "Processing file 7: overview-with-streaming.txt\n",
      "Processing file 8: libraries.txt\n",
      "Processing file 9: tier-four.txt\n",
      "Processing file 10: curl-setup.txt\n",
      "Processing file 11: crawl-website-embeddings.txt\n",
      "Processing file 12: error-codes.txt\n",
      "Generated title embeddings for images-node-tips.txt\n",
      "Generated title embeddings for crawl-website-embeddings.txt\n",
      "Generated title embeddings for overview-with-streaming.txt\n",
      "Generated title embeddings for libraries.txt\n",
      "Generated title embeddings for latency-optimization.txt\n",
      "Generated title embeddings for tier-two.txt\n",
      "Generated title embeddings for hackathons.txt\n",
      "Generated title embeddings for fine-tuning.txt\n",
      "Generated title embeddings for tier-four.txt\n",
      "Generated title embeddings for curl-setup.txt\n",
      "Generated content embeddings for hackathons.txt\n",
      "Generated title embeddings for error-codes.txt\n",
      "Generated title embeddings for function-calling-run-example--polling.txt\n",
      "Generated content embeddings for curl-setup.txt\n",
      "Generated content embeddings for tier-four.txt\n",
      "Generated content embeddings for images-node-tips.txt\n",
      "Generated content embeddings for tier-two.txt\n",
      "Generated content embeddings for overview-with-streaming.txt\n",
      "Generated content embeddings for libraries.txt\n",
      "Generated content embeddings for latency-optimization.txt\n",
      "Generated content embeddings for crawl-website-embeddings.txt\n",
      "Generated content embeddings for function-calling-run-example--polling.txt\n",
      "Categorized hackathons.txt as other\n",
      "Appended data for chunk 1/1 of hackathons.txt\n",
      "Processing file 13: images-python-tips.txt\n",
      "Categorized tier-four.txt as billing_limits\n",
      "Appended data for chunk 1/1 of tier-four.txt\n",
      "Processing file 14: batch.txt\n",
      "Generated content embeddings for error-codes.txt\n",
      "Categorized curl-setup.txt as setup\n",
      "Appended data for chunk 1/1 of curl-setup.txt\n",
      "Processing file 15: meeting-minutes-tutorial.txt\n",
      "Generated title embeddings for images-python-tips.txt\n",
      "Categorized overview-with-streaming.txt as tools\n",
      "Appended data for chunk 1/1 of overview-with-streaming.txt\n",
      "Processing file 16: moderation.txt\n",
      "Categorized function-calling-run-example--polling.txt as tools\n",
      "Appended data for chunk 1/1 of function-calling-run-example--polling.txt\n",
      "Processing file 17: models.txt\n",
      "Generated title embeddings for batch.txt\n",
      "Categorized images-node-tips.txt as tools\n",
      "Appended data for chunk 1/1 of images-node-tips.txt\n",
      "Processing file 18: release-notes.txt\n",
      "Categorized tier-two.txt as billing_limits\n",
      "Appended data for chunk 1/1 of tier-two.txt\n",
      "Processing file 19: data-retrieval.txt\n",
      "Generated title embeddings for models.txt\n",
      "Generated title embeddings for moderation.txt\n",
      "Generated title embeddings for meeting-minutes-tutorial.txt\n",
      "Generated content embeddings for images-python-tips.txt\n",
      "Generated title embeddings for release-notes.txt\n",
      "Categorized crawl-website-embeddings.txt as techniques\n",
      "Appended data for chunk 1/1 of crawl-website-embeddings.txt\n",
      "Processing file 20: production-best-practices.txt\n",
      "Categorized latency-optimization.txt as techniques\n",
      "Appended data for chunk 1/1 of latency-optimization.txt\n",
      "Processing file 21: index.txt\n",
      "Generated title embeddings for data-retrieval.txt\n",
      "Categorized libraries.txt as tools\n",
      "Appended data for chunk 1/1 of libraries.txt\n",
      "Processing file 22: authentication.txt\n",
      "Categorized error-codes.txt as authentication\n",
      "Appended data for chunk 1/1 of error-codes.txt\n",
      "Processing file 23: function-calling.txt\n",
      "Generated content embeddings for release-notes.txt\n",
      "Generated title embeddings for production-best-practices.txt\n",
      "Generated content embeddings for meeting-minutes-tutorial.txt\n",
      "Categorized images-python-tips.txt as tools\n",
      "Appended data for chunk 1/1 of images-python-tips.txt\n",
      "Processing file 24: embeddings.txt\n",
      "Generated content embeddings for models.txt\n",
      "Generated content embeddings for data-retrieval.txt\n",
      "Generated title embeddings for authentication.txt\n",
      "Generated content embeddings for fine-tuning.txt\n",
      "Generated title embeddings for index.txt\n",
      "Generated title embeddings for embeddings.txt\n",
      "Categorized release-notes.txt as other\n",
      "Appended data for chunk 1/1 of release-notes.txt\n",
      "Processing file 25: vision.txt\n",
      "Generated content embeddings for production-best-practices.txt\n",
      "Generated content embeddings for index.txt\n",
      "Generated content embeddings for moderation.txt\n",
      "Generated title embeddings for vision.txt\n",
      "Generated title embeddings for function-calling.txt\n",
      "Categorized data-retrieval.txt as techniques\n",
      "Appended data for chunk 1/1 of data-retrieval.txt\n",
      "Processing file 26: text-generation.txt\n",
      "Categorized models.txt as models\n",
      "Appended data for chunk 1/1 of models.txt\n",
      "Processing file 27: getting-started.txt\n",
      "Categorized production-best-practices.txt as other\n",
      "Appended data for chunk 1/1 of production-best-practices.txt\n",
      "Processing file 28: tool-function-calling.txt\n",
      "Generated title embeddings for text-generation.txt\n",
      "Generated title embeddings for getting-started.txt\n",
      "Categorized fine-tuning.txt as techniques\n",
      "Appended data for chunk 1/2 of fine-tuning.txt\n",
      "Appended data for chunk 2/2 of fine-tuning.txt\n",
      "Processing file 29: migration.txt\n",
      "Categorized index.txt as setup\n",
      "Appended data for chunk 1/1 of index.txt\n",
      "Processing file 30: text-to-speech.txt\n",
      "Generated content embeddings for function-calling.txt\n",
      "Generated title embeddings for tool-function-calling.txt\n",
      "Categorized moderation.txt as techniques\n",
      "Appended data for chunk 1/1 of moderation.txt\n",
      "Processing file 31: speech-to-text.txt\n",
      "Generated content embeddings for batch.txt\n",
      "Generated content embeddings for authentication.txt\n",
      "Generated title embeddings for migration.txt\n",
      "Categorized meeting-minutes-tutorial.txt as techniques\n",
      "Appended data for chunk 1/1 of meeting-minutes-tutorial.txt\n",
      "Processing file 32: optimizing-llm-accuracy.txt\n",
      "Generated title embeddings for text-to-speech.txt\n",
      "Generated content embeddings for getting-started.txt\n",
      "Generated title embeddings for optimizing-llm-accuracy.txt\n",
      "Generated title embeddings for speech-to-text.txt\n",
      "Generated content embeddings for text-to-speech.txt\n",
      "Categorized authentication.txt as authentication\n",
      "Appended data for chunk 1/1 of authentication.txt\n",
      "Processing file 33: images.txt\n",
      "Generated content embeddings for migration.txt\n",
      "Categorized function-calling.txt as tools\n",
      "Appended data for chunk 1/1 of function-calling.txt\n",
      "Processing file 34: introduction.txt\n",
      "Categorized batch.txt as tools\n",
      "Appended data for chunk 1/1 of batch.txt\n",
      "Processing file 35: tier-free.txt\n",
      "Generated title embeddings for images.txt\n",
      "Generated content embeddings for vision.txt\n",
      "Categorized getting-started.txt as setup\n",
      "Appended data for chunk 1/1 of getting-started.txt\n",
      "Processing file 36: changelog.txt\n",
      "Generated content embeddings for embeddings.txt\n",
      "Generated title embeddings for introduction.txt\n",
      "Generated title embeddings for tier-free.txt\n",
      "Generated content embeddings for images.txt\n",
      "Generated title embeddings for changelog.txt\n",
      "Generated content embeddings for speech-to-text.txt\n",
      "Categorized text-to-speech.txt as models\n",
      "Appended data for chunk 1/1 of text-to-speech.txt\n",
      "Processing file 37: prompt-engineering.txt\n",
      "Categorized migration.txt as tools\n",
      "Appended data for chunk 1/1 of migration.txt\n",
      "Processing file 38: overview-without-streaming.txt\n",
      "Generated content embeddings for tier-free.txt\n",
      "Generated content embeddings for tool-function-calling.txt\n",
      "Generated content embeddings for introduction.txt\n",
      "Generated content embeddings for optimizing-llm-accuracy.txt\n",
      "Generated title embeddings for prompt-engineering.txt\n",
      "Categorized embeddings.txt as models\n",
      "Appended data for chunk 1/1 of embeddings.txt\n",
      "Processing file 39: how-it-works.txt\n",
      "Categorized images.txt as tools\n",
      "Appended data for chunk 1/1 of images.txt\n",
      "Processing file 40: gptbot.txt\n",
      "Categorized vision.txt as models\n",
      "Appended data for chunk 1/1 of vision.txt\n",
      "Processing file 41: tier-one.txt\n",
      "Generated title embeddings for overview-without-streaming.txt\n",
      "Generated content embeddings for changelog.txt\n",
      "Generated title embeddings for how-it-works.txt\n",
      "Generated content embeddings for text-generation.txt\n",
      "Generated title embeddings for gptbot.txt\n",
      "Categorized speech-to-text.txt as tools\n",
      "Appended data for chunk 1/1 of speech-to-text.txt\n",
      "Processing file 42: tool-code-interpreter.txt\n",
      "Categorized changelog.txt as other\n",
      "Appended data for chunk 1/1 of changelog.txt\n",
      "Processing file 43: python-setup.txt\n",
      "Categorized introduction.txt as techniques\n",
      "Appended data for chunk 1/1 of introduction.txt\n",
      "Processing file 44: supported-countries.txt\n",
      "Categorized optimizing-llm-accuracy.txt as techniques\n",
      "Appended data for chunk 1/1 of optimizing-llm-accuracy.txt\n",
      "Processing file 45: production.txt\n",
      "Categorized tier-free.txt as billing_limits\n",
      "Appended data for chunk 1/1 of tier-free.txt\n",
      "Processing file 46: tier-five.txt\n",
      "Generated title embeddings for python-setup.txt\n",
      "Generated title embeddings for tier-five.txt\n",
      "Generated content embeddings for gptbot.txt\n",
      "Generated title embeddings for supported-countries.txt\n",
      "Generated title embeddings for tool-code-interpreter.txt\n",
      "Categorized tool-function-calling.txt as techniques\n",
      "Appended data for chunk 1/1 of tool-function-calling.txt\n",
      "Processing file 47: tier-three.txt\n",
      "Categorized text-generation.txt as models\n",
      "Appended data for chunk 1/1 of text-generation.txt\n",
      "Processing file 48: tool-file-search.txt\n",
      "Generated content embeddings for supported-countries.txt\n",
      "Generated title embeddings for production.txt\n",
      "Generated title embeddings for tier-three.txt\n",
      "Generated title embeddings for tool-file-search.txt\n",
      "Generated content embeddings for tier-five.txt\n",
      "Categorized gptbot.txt as tools\n",
      "Appended data for chunk 1/1 of gptbot.txt\n",
      "Processing file 49: node-setup.txt\n",
      "Generated content embeddings for python-setup.txt\n",
      "Generated content embeddings for overview-without-streaming.txt\n",
      "Generated content embeddings for production.txt\n",
      "Generated title embeddings for node-setup.txt\n",
      "Generated content embeddings for tier-three.txt\n",
      "Generated content embeddings for tool-code-interpreter.txt\n",
      "Generated content embeddings for prompt-engineering.txt\n",
      "Categorized tier-five.txt as billing_limits\n",
      "Appended data for chunk 1/1 of tier-five.txt\n",
      "Processing file 50: whats-new.txt\n",
      "Generated content embeddings for how-it-works.txt\n",
      "Categorized python-setup.txt as setup\n",
      "Appended data for chunk 1/1 of python-setup.txt\n",
      "Processing file 51: function-calling-run-example--streaming.txt\n",
      "Generated title embeddings for whats-new.txt\n",
      "Generated content embeddings for tool-file-search.txt\n",
      "Generated title embeddings for function-calling-run-example--streaming.txt\n",
      "Categorized production.txt as setup\n",
      "Appended data for chunk 1/1 of production.txt\n",
      "Processing file 52: deprecations.txt\n",
      "Categorized tier-three.txt as billing_limits\n",
      "Appended data for chunk 1/1 of tier-three.txt\n",
      "Processing file 53: safety-best-practices.txt\n",
      "Generated content embeddings for node-setup.txt\n",
      "Categorized prompt-engineering.txt as techniques\n",
      "Appended data for chunk 1/2 of prompt-engineering.txt\n",
      "Appended data for chunk 2/2 of prompt-engineering.txt\n",
      "Categorized overview-without-streaming.txt as techniques\n",
      "Appended data for chunk 1/1 of overview-without-streaming.txt\n",
      "Generated title embeddings for tier-one.txt\n",
      "Generated content embeddings for whats-new.txt\n",
      "Categorized how-it-works.txt as tools\n",
      "Appended data for chunk 1/1 of how-it-works.txt\n",
      "Generated title embeddings for deprecations.txt\n",
      "Categorized tool-code-interpreter.txt as tools\n",
      "Appended data for chunk 1/1 of tool-code-interpreter.txt\n",
      "Generated content embeddings for function-calling-run-example--streaming.txt\n",
      "Generated content embeddings for tier-one.txt\n",
      "Categorized supported-countries.txt as setup\n",
      "Appended data for chunk 1/1 of supported-countries.txt\n",
      "Generated title embeddings for safety-best-practices.txt\n",
      "Categorized node-setup.txt as setup\n",
      "Appended data for chunk 1/1 of node-setup.txt\n",
      "Categorized tool-file-search.txt as tools\n",
      "Appended data for chunk 1/1 of tool-file-search.txt\n",
      "Generated content embeddings for safety-best-practices.txt\n",
      "Generated content embeddings for deprecations.txt\n",
      "Categorized function-calling-run-example--streaming.txt as tools\n",
      "Appended data for chunk 1/1 of function-calling-run-example--streaming.txt\n",
      "Categorized whats-new.txt as tools\n",
      "Appended data for chunk 1/1 of whats-new.txt\n",
      "Categorized tier-one.txt as billing_limits\n",
      "Appended data for chunk 1/1 of tier-one.txt\n",
      "Categorized safety-best-practices.txt as techniques\n",
      "Appended data for chunk 1/1 of safety-best-practices.txt\n",
      "Categorized deprecations.txt as models\n",
      "Appended data for chunk 1/1 of deprecations.txt\n",
      "Wrote row with id 3_0 to CSV\n",
      "Wrote row with id 8_0 to CSV\n",
      "Wrote row with id 9_0 to CSV\n",
      "Wrote row with id 6_0 to CSV\n",
      "Wrote row with id 1_0 to CSV\n",
      "Wrote row with id 5_0 to CSV\n",
      "Wrote row with id 4_0 to CSV\n",
      "Wrote row with id 10_0 to CSV\n",
      "Wrote row with id 0_0 to CSV\n",
      "Wrote row with id 7_0 to CSV\n",
      "Wrote row with id 11_0 to CSV\n",
      "Wrote row with id 12_0 to CSV\n",
      "Wrote row with id 17_0 to CSV\n",
      "Wrote row with id 18_0 to CSV\n",
      "Wrote row with id 16_0 to CSV\n",
      "Wrote row with id 19_0 to CSV\n",
      "Wrote row with id 2_0 to CSV\n",
      "Wrote row with id 2_1 to CSV\n",
      "Wrote row with id 20_0 to CSV\n",
      "Wrote row with id 15_0 to CSV\n",
      "Wrote row with id 14_0 to CSV\n",
      "Wrote row with id 21_0 to CSV\n",
      "Wrote row with id 22_0 to CSV\n",
      "Wrote row with id 13_0 to CSV\n",
      "Wrote row with id 26_0 to CSV\n",
      "Wrote row with id 29_0 to CSV\n",
      "Wrote row with id 28_0 to CSV\n",
      "Wrote row with id 23_0 to CSV\n",
      "Wrote row with id 32_0 to CSV\n",
      "Wrote row with id 24_0 to CSV\n",
      "Wrote row with id 30_0 to CSV\n",
      "Wrote row with id 35_0 to CSV\n",
      "Wrote row with id 33_0 to CSV\n",
      "Wrote row with id 31_0 to CSV\n",
      "Wrote row with id 34_0 to CSV\n",
      "Wrote row with id 27_0 to CSV\n",
      "Wrote row with id 25_0 to CSV\n",
      "Wrote row with id 39_0 to CSV\n",
      "Wrote row with id 45_0 to CSV\n",
      "Wrote row with id 42_0 to CSV\n",
      "Wrote row with id 44_0 to CSV\n",
      "Wrote row with id 46_0 to CSV\n",
      "Wrote row with id 36_0 to CSV\n",
      "Wrote row with id 36_1 to CSV\n",
      "Wrote row with id 37_0 to CSV\n",
      "Wrote row with id 38_0 to CSV\n",
      "Wrote row with id 41_0 to CSV\n",
      "Wrote row with id 43_0 to CSV\n",
      "Wrote row with id 48_0 to CSV\n",
      "Wrote row with id 47_0 to CSV\n",
      "Wrote row with id 50_0 to CSV\n",
      "Wrote row with id 49_0 to CSV\n",
      "Wrote row with id 40_0 to CSV\n",
      "Wrote row with id 52_0 to CSV\n",
      "Wrote row with id 51_0 to CSV\n",
      "dataset oai_docs already exists\n",
      "Table cse-144-project.oai_docs.embedded_data_0 already exists\n",
      "Table cse-144-project.oai_docs.embedded_data_1 already exists\n",
      "Table cse-144-project.oai_docs.embedded_data_2 does not exist\n",
      "Created final table cse-144-project.oai_docs.embedded_data_2\n",
      "Successfully loaded data into cse-144-project.oai_docs:cse-144-project.oai_docs.embedded_data\n"
     ]
    }
   ],
   "source": [
    "## Customize the location below if you are using different data besides the OpenAI documentation. Note that if you are using a different dataset, you will need to update the categories list as well.\n",
    "folder_name = \"data\"\n",
    "\n",
    "files = [\n",
    "    os.path.join(folder_name, f)\n",
    "    for f in os.listdir(folder_name)\n",
    "    if f.endswith(\".txt\") or f.endswith(\".pdf\")\n",
    "]\n",
    "data = []\n",
    "\n",
    "# Process each file concurrently\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            process_file, file_path, idx, categories, embeddings_model\n",
    "        ): idx\n",
    "        for idx, file_path in enumerate(files)\n",
    "    }\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            data.extend(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "# Write the data to a CSV file\n",
    "csv_file = \"embedded_data.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\n",
    "        \"id\",\n",
    "        \"vector_id\",\n",
    "        \"title\",\n",
    "        \"text\",\n",
    "        \"title_vector\",\n",
    "        \"content_vector\",\n",
    "        \"category\",\n",
    "    ]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "        print(f\"Wrote row with id {row['id']} to CSV\")\n",
    "\n",
    "# Convert the CSV file to a Dataframe\n",
    "article_df = pd.read_csv(\n",
    "    csv_file,\n",
    ")\n",
    "# Read vectors from strings back into a list using json.loads\n",
    "article_df[\"title_vector\"] = article_df.title_vector.apply(json.loads)\n",
    "article_df[\"content_vector\"] = article_df.content_vector.apply(json.loads)\n",
    "article_df[\"vector_id\"] = article_df[\"vector_id\"].apply(str)\n",
    "article_df[\"category\"] = article_df[\"category\"].apply(str)\n",
    "article_df.head()\n",
    "\n",
    "# Define the dataset ID (project_id.dataset_id)\n",
    "raw_dataset_id = \"oai_docs\"\n",
    "dataset_id = project_id + \".\" + raw_dataset_id\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Construct a full Dataset object to send to the API\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "# Specify the geographic location where the dataset should reside\n",
    "dataset.location = \"US\"\n",
    "\n",
    "# Send the dataset to the API for creation\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print(f\"Created dataset {client.project}.{dataset.dataset_id}\")\n",
    "except Conflict:\n",
    "    print(f\"dataset {dataset.dataset_id } already exists\")\n",
    "\n",
    "# Read the CSV file, properly handling multiline fields\n",
    "df = pd.read_csv(csv_file, engine=\"python\", quotechar='\"', quoting=1)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()\n",
    "\n",
    "# Preprocess the data to ensure content_vector is correctly formatted\n",
    "# removing last and first character which are brackets [], comma splitting and converting to float\n",
    "def preprocess_content_vector(row):\n",
    "    row[\"content_vector\"] = [\n",
    "        float(x) for x in row[\"content_vector\"][1:-1].split(\",\")\n",
    "    ]\n",
    "    return row\n",
    "\n",
    "# Apply preprocessing to the dataframe\n",
    "df = df.apply(preprocess_content_vector, axis=1)\n",
    "\n",
    "# Define the schema of the final table\n",
    "final_schema = [\n",
    "    bigquery.SchemaField(\"id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"vector_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"title\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"text\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"title_vector\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"content_vector\", \"FLOAT64\", mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"category\", \"STRING\"),\n",
    "]\n",
    "\n",
    "# Define the final table ID\n",
    "raw_table_id = \"embedded_data\"\n",
    "final_table_id = f\"{dataset_id}.\" + raw_table_id\n",
    "\n",
    "# Create the final table object\n",
    "idx = -1\n",
    "found = False\n",
    "\n",
    "while not found:\n",
    "    idx += 1\n",
    "    try:\n",
    "        final_table = bigquery.Table(f\"{final_table_id}_{idx}\", schema=final_schema)\n",
    "        client.get_table(final_table)\n",
    "        print(f\"Table {final_table_id}_{idx} already exists\")\n",
    "    except Exception as e:\n",
    "        found = True\n",
    "        print(f\"Table {final_table_id}_{idx} does not exist\")\n",
    "\n",
    "\n",
    "# Send the table to the API for creation\n",
    "final_table = client.create_table(final_table, exists_ok=True)  # API request\n",
    "print(\n",
    "    f\"Created final table {project_id}.{final_table.dataset_id}.{final_table.table_id}\"\n",
    ")\n",
    "\n",
    "# Convert DataFrame to list of dictionaries for BigQuery insertion\n",
    "rows_to_insert = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Upload data to the final table\n",
    "errors = client.insert_rows_json(\n",
    "    f\"{final_table.dataset_id}.{final_table.table_id}\", rows_to_insert\n",
    ")  # API request\n",
    "\n",
    "if errors:\n",
    "    print(f\"Encountered errors while inserting rows: {errors}\")\n",
    "else:\n",
    "    print(f\"Successfully loaded data into {dataset_id}:{final_table_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
