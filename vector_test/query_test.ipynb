{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "from itertools import islice\n",
    "import concurrent.futures\n",
    "import yaml\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import pyperclip\n",
    "\n",
    "# OpenAI Libraries\n",
    "from openai import OpenAI\n",
    "\n",
    "# Google Cloud Identity and Credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import functions_v1\n",
    "from google.api_core.exceptions import Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this as a variable to reference in function app in later step\n",
    "openai_api_key = json.load(\n",
    "    open(\"openai.json\")\n",
    ")[\"key\"]\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "embeddings_model = \"text-embedding-3-small\"  # We'll use this by default, but you can change to your text-embedding-3-large if desired\n",
    "\n",
    "# Use default credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    # os.path.join(os.path.dirname(os.path.abspath(__file__)), \"google.json\")\n",
    "    \"google.json\"\n",
    ")\n",
    "project_id = \"cse-144-project\"\n",
    "region = \"us-central1\"  # e.g: \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n",
    "    # batched('ABCDEFG', 3) --> ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be at least one\")\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def chunked_tokens(text, chunk_length, encoding_name=\"cl100k_base\"):\n",
    "    # Get the encoding object for the specified encoding name. OpenAI's tiktoken library, which is used in this notebook, currently supports two encodings: 'bpe' and 'cl100k_base'. The 'bpe' encoding is used for GPT-3 and earlier models, while 'cl100k_base' is used for newer models like GPT-4.\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # Encode the input text into tokens\n",
    "    tokens = encoding.encode(text)\n",
    "    # Create an iterator that yields chunks of tokens of the specified length\n",
    "    chunks_iterator = batched(tokens, chunk_length)\n",
    "    # Yield each chunk from the iterator\n",
    "    yield from chunks_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING = \"cl100k_base\"\n",
    "\n",
    "\n",
    "def generate_embeddings(text, model):\n",
    "    # Generate embeddings for the provided text using the specified model\n",
    "    embeddings_response = openai_client.embeddings.create(model=model, input=text)\n",
    "    # Extract the embedding data from the response\n",
    "    embedding = embeddings_response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def len_safe_get_embedding(\n",
    "    text,\n",
    "    model=embeddings_model,\n",
    "    max_tokens=EMBEDDING_CTX_LENGTH,\n",
    "    encoding_name=EMBEDDING_ENCODING,\n",
    "):\n",
    "    # Initialize lists to store embeddings and corresponding text chunks\n",
    "    chunk_embeddings = []\n",
    "    chunk_texts = []\n",
    "    # Iterate over chunks of tokens from the input text\n",
    "    for chunk in chunked_tokens(\n",
    "        text, chunk_length=max_tokens, encoding_name=encoding_name\n",
    "    ):\n",
    "        # Generate embeddings for each chunk and append to the list\n",
    "        chunk_embeddings.append(generate_embeddings(chunk, model=model))\n",
    "        # Decode the chunk back to text and append to the list\n",
    "        chunk_texts.append(tiktoken.get_encoding(encoding_name).decode(chunk))\n",
    "    # Return the list of chunk embeddings and the corresponding text chunks\n",
    "    return chunk_embeddings, chunk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"authentication\",\n",
    "    \"models\",\n",
    "    \"techniques\",\n",
    "    \"tools\",\n",
    "    \"setup\",\n",
    "    \"billing_limits\",\n",
    "    \"other\",\n",
    "]\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is prompt engineering?\n",
      "query_id: query_vector, base_id: 0_0, distance: 0.67017727196806, text_truncated: \n",
      "\n",
      "# Latency optimization\n",
      "\n",
      "This guide covers the core set of principles you can apply to improve late\n",
      "\n",
      "query_id: query_vector, base_id: 36_0, distance: 0.7026942763690709, text_truncated: \n",
      "\n",
      "Prompt engineering\n",
      "\n",
      "This guide shares strategies and tactics for getting better results from large\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is prompt engineering?\"\n",
    "print(f\"Query: {query}\")\n",
    "category = \"models\"\n",
    "\n",
    "embedding_query = generate_embeddings(query, embeddings_model)\n",
    "embedding_query_list = \", \".join(map(str, embedding_query))\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH search_results AS (\n",
    "  SELECT query.id AS query_id, base.id AS base_id, distance\n",
    "  FROM VECTOR_SEARCH(\n",
    "    TABLE oai_docs.embedded_data, 'content_vector',\n",
    "    (SELECT ARRAY[{embedding_query_list}] AS content_vector, 'query_vector' AS id),\n",
    "    top_k => 2, distance_type => 'COSINE', options => '{{\"use_brute_force\": true}}')\n",
    ")\n",
    "SELECT sr.query_id, sr.base_id, sr.distance, ed.text, ed.title\n",
    "FROM search_results sr\n",
    "JOIN oai_docs.embedded_data ed ON sr.base_id = ed.id\n",
    "ORDER BY sr.distance ASC\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()  # Wait for the job to complete\n",
    "\n",
    "for row in results:\n",
    "    print(\n",
    "        f\"query_id: {row['query_id']}, base_id: {row['base_id']}, distance: {row['distance']}, text_truncated: {row['text'][0:100]}\"\n",
    "    )\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
